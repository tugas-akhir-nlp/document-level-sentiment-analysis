{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Concatenate\n",
    "import keras.layers.merge\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Doc2Vec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 500 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = 175303 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = 100 # max number of words in a comment to use\n",
    "\n",
    "#training params\n",
    "batch_size = 256  \n",
    "num_epochs = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r'\\d+','<number>',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gerindra alihkan rekomendasi ke agus an tanri ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cuci tangan pakai sunlight stelah itu pakai sa...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kasus toko obat digerebek fpi propam akan peri...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>menkeu melemah nya rupiah lebih berpengaruh pa...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>minyak jarak castor oil &lt;number&gt; ml</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content polarity\n",
       "0  gerindra alihkan rekomendasi ke agus an tanri ...  neutral\n",
       "1  cuci tangan pakai sunlight stelah itu pakai sa...  neutral\n",
       "2  kasus toko obat digerebek fpi propam akan peri...  neutral\n",
       "3  menkeu melemah nya rupiah lebih berpengaruh pa...  neutral\n",
       "4                minyak jarak castor oil <number> ml  neutral"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_comments = pd.read_csv(\"./corpus/prosa/data_clean_punctuation/data_train_full.csv\")\n",
    "clean_train_comments['content'] = clean_train_comments['content'].apply(preprocess)\n",
    "clean_train_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gerindra alihkan rekomendasi ke agus an tanri ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[gerindra, alihkan, rekomendasi, ke, agus, an,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cuci tangan pakai sunlight stelah itu pakai sa...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[cuci, tangan, pakai, sunlight, stelah, itu, p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kasus toko obat digerebek fpi propam akan peri...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[kasus, toko, obat, digerebek, fpi, propam, ak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>menkeu melemah nya rupiah lebih berpengaruh pa...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[menkeu, melemah, nya, rupiah, lebih, berpenga...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>minyak jarak castor oil &lt;number&gt; ml</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[minyak, jarak, castor, oil, number, ml]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content polarity  \\\n",
       "0  gerindra alihkan rekomendasi ke agus an tanri ...  neutral   \n",
       "1  cuci tangan pakai sunlight stelah itu pakai sa...  neutral   \n",
       "2  kasus toko obat digerebek fpi propam akan peri...  neutral   \n",
       "3  menkeu melemah nya rupiah lebih berpengaruh pa...  neutral   \n",
       "4                minyak jarak castor oil <number> ml  neutral   \n",
       "\n",
       "                                              tokens  sentiment  \n",
       "0  [gerindra, alihkan, rekomendasi, ke, agus, an,...          1  \n",
       "1  [cuci, tangan, pakai, sunlight, stelah, itu, p...          1  \n",
       "2  [kasus, toko, obat, digerebek, fpi, propam, ak...          1  \n",
       "3  [menkeu, melemah, nya, rupiah, lebih, berpenga...          1  \n",
       "4           [minyak, jarak, castor, oil, number, ml]          1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "clean_train_comments['content'] = clean_train_comments['content'].astype('str') \n",
    "clean_train_comments.dtypes\n",
    "clean_train_comments['tokens'] = clean_train_comments['content'].apply(tokenizer.tokenize)\n",
    "clean_train_comments['sentiment'] = clean_train_comments['polarity'].astype('category').cat.codes\n",
    "   \n",
    "clean_train_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kemarin gue datang ke tempat makan baru yang a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kayak nya sih gue tidak akan mau balik lagi ke...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kalau dipikirpikir sebenarnya tidak ada yang b...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ini pertama kalinya gua ke bank buat ngurusin ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>waktu sampai dengan gue pernah disuruh ibu lat...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  polarity\n",
       "0  kemarin gue datang ke tempat makan baru yang a...  negative\n",
       "1  kayak nya sih gue tidak akan mau balik lagi ke...  negative\n",
       "2  kalau dipikirpikir sebenarnya tidak ada yang b...  negative\n",
       "3  ini pertama kalinya gua ke bank buat ngurusin ...  negative\n",
       "4  waktu sampai dengan gue pernah disuruh ibu lat...  negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_comments = pd.read_csv(\"./corpus/prosa/data_clean_punctuation/data_testing_full.csv\")\n",
    "clean_test_comments['content'] = clean_test_comments['content'].apply(preprocess)\n",
    "clean_test_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kemarin gue datang ke tempat makan baru yang a...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[kemarin, gue, datang, ke, tempat, makan, baru...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kayak nya sih gue tidak akan mau balik lagi ke...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[kayak, nya, sih, gue, tidak, akan, mau, balik...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kalau dipikirpikir sebenarnya tidak ada yang b...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[kalau, dipikirpikir, sebenarnya, tidak, ada, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ini pertama kalinya gua ke bank buat ngurusin ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[ini, pertama, kalinya, gua, ke, bank, buat, n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>waktu sampai dengan gue pernah disuruh ibu lat...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[waktu, sampai, dengan, gue, pernah, disuruh, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  polarity  \\\n",
       "0  kemarin gue datang ke tempat makan baru yang a...  negative   \n",
       "1  kayak nya sih gue tidak akan mau balik lagi ke...  negative   \n",
       "2  kalau dipikirpikir sebenarnya tidak ada yang b...  negative   \n",
       "3  ini pertama kalinya gua ke bank buat ngurusin ...  negative   \n",
       "4  waktu sampai dengan gue pernah disuruh ibu lat...  negative   \n",
       "\n",
       "                                              tokens  sentiment  \n",
       "0  [kemarin, gue, datang, ke, tempat, makan, baru...          0  \n",
       "1  [kayak, nya, sih, gue, tidak, akan, mau, balik...          0  \n",
       "2  [kalau, dipikirpikir, sebenarnya, tidak, ada, ...          0  \n",
       "3  [ini, pertama, kalinya, gua, ke, bank, buat, n...          0  \n",
       "4  [waktu, sampai, dengan, gue, pernah, disuruh, ...          0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_comments['content'] = clean_test_comments['content'].astype('str') \n",
    "clean_test_comments.dtypes\n",
    "clean_test_comments[\"tokens\"] = clean_test_comments[\"content\"].apply(tokenizer.tokenize)\n",
    "clean_test_comments['sentiment'] = clean_test_comments['polarity'].astype('category').cat.codes\n",
    "\n",
    "clean_test_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235162 words total, with a vocabulary size of 16127\n",
      "Max sentence length is 84\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in clean_train_comments[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in clean_train_comments[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10767 words total, with a vocabulary size of 2692\n",
      "Max sentence length is 72\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in clean_test_comments[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in clean_test_comments[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load('./prosa-w2v/prosa.vec')\n",
    "# word2vec = Word2Vec.load('./vectorizer/prosa/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16125 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(clean_train_comments[\"content\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(clean_train_comments[\"content\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(clean_test_comments[\"content\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False, extra_conv=True):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    #l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.5)(l_merge)  \n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc']) \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_tr = clean_train_comments['sentiment'].values\n",
    "y_ts = clean_test_comments['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_train = y_tr\n",
    "\n",
    "x_test = test_cnn_data\n",
    "y_test = y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 700)     11288200    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 98, 128)      268928      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 97, 128)      358528      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 96, 128)      448128      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 32, 128)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 32, 128)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 32, 128)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 96, 128)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 96, 128)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 12288)        0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          1572992     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            387         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 13,937,163\n",
      "Trainable params: 2,648,963\n",
      "Non-trainable params: 11,288,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define callbacks\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=4, verbose=1)\n",
    "# callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8939 samples, validate on 500 samples\n",
      "Epoch 1/20\n",
      "8939/8939 [==============================] - 71s 8ms/step - loss: 1.2240 - acc: 0.6528 - val_loss: 1.0219 - val_acc: 0.5440\n",
      "Epoch 2/20\n",
      "8939/8939 [==============================] - 72s 8ms/step - loss: 0.5424 - acc: 0.7917 - val_loss: 0.9846 - val_acc: 0.5820\n",
      "Epoch 3/20\n",
      "8939/8939 [==============================] - 71s 8ms/step - loss: 0.4242 - acc: 0.8410 - val_loss: 0.8967 - val_acc: 0.6500\n",
      "Epoch 4/20\n",
      "8939/8939 [==============================] - 71s 8ms/step - loss: 0.3258 - acc: 0.8782 - val_loss: 0.7898 - val_acc: 0.7040\n",
      "Epoch 5/20\n",
      "8939/8939 [==============================] - 71s 8ms/step - loss: 0.2613 - acc: 0.9033 - val_loss: 0.8236 - val_acc: 0.7360\n",
      "Epoch 6/20\n",
      "8939/8939 [==============================] - 72s 8ms/step - loss: 0.1980 - acc: 0.9282 - val_loss: 0.6431 - val_acc: 0.7820\n",
      "Epoch 7/20\n",
      "8939/8939 [==============================] - 73s 8ms/step - loss: 0.1643 - acc: 0.9414 - val_loss: 0.9111 - val_acc: 0.7400\n",
      "Epoch 8/20\n",
      "8939/8939 [==============================] - 73s 8ms/step - loss: 0.1304 - acc: 0.9535 - val_loss: 0.9864 - val_acc: 0.7480\n",
      "Epoch 9/20\n",
      "8939/8939 [==============================] - 72s 8ms/step - loss: 0.1108 - acc: 0.9602 - val_loss: 1.0769 - val_acc: 0.7440\n",
      "Epoch 10/20\n",
      "8939/8939 [==============================] - 72s 8ms/step - loss: 0.0989 - acc: 0.9631 - val_loss: 0.9678 - val_acc: 0.7500\n",
      "Epoch 11/20\n",
      "8939/8939 [==============================] - 72s 8ms/step - loss: 0.0853 - acc: 0.9704 - val_loss: 0.9575 - val_acc: 0.7600\n",
      "Epoch 12/20\n",
      "8939/8939 [==============================] - 72s 8ms/step - loss: 0.0750 - acc: 0.9761 - val_loss: 0.9735 - val_acc: 0.7780\n",
      "Epoch 13/20\n",
      "8939/8939 [==============================] - 72s 8ms/step - loss: 0.0764 - acc: 0.9745 - val_loss: 1.0521 - val_acc: 0.7640\n",
      "Epoch 14/20\n",
      "8939/8939 [==============================] - 72s 8ms/step - loss: 0.0697 - acc: 0.9747 - val_loss: 1.1271 - val_acc: 0.7340\n",
      "Epoch 15/20\n",
      "8939/8939 [==============================] - 72s 8ms/step - loss: 0.0633 - acc: 0.9784 - val_loss: 1.1763 - val_acc: 0.7600\n",
      "Epoch 16/20\n",
      "8939/8939 [==============================] - 72s 8ms/step - loss: 0.0576 - acc: 0.9806 - val_loss: 1.3499 - val_acc: 0.7320\n",
      "Epoch 17/20\n",
      "8939/8939 [==============================] - 72s 8ms/step - loss: 0.0547 - acc: 0.9806 - val_loss: 1.0863 - val_acc: 0.7760\n",
      "Epoch 18/20\n",
      "8939/8939 [==============================] - 72s 8ms/step - loss: 0.0449 - acc: 0.9858 - val_loss: 1.1165 - val_acc: 0.7740\n",
      "Epoch 19/20\n",
      "8939/8939 [==============================] - 72s 8ms/step - loss: 0.0534 - acc: 0.9815 - val_loss: 1.6737 - val_acc: 0.7200\n",
      "Epoch 20/20\n",
      "8939/8939 [==============================] - 72s 8ms/step - loss: 0.0559 - acc: 0.9811 - val_loss: 1.2094 - val_acc: 0.7700\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, to_categorical(y_train), epochs=num_epochs, validation_data=(x_test, to_categorical(y_test)), batch_size=batch_size) #callbacks=callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save('./model/yoon_kim_3/cnn_model_04.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 2s 3ms/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0  0.74308300 0.92156863 0.82275711       204\n",
      "          1  0.66197183 0.53409091 0.59119497        88\n",
      "          2  0.85227273 0.72115385 0.78125000       208\n",
      "\n",
      "avg / total  0.77423036 0.77000000 0.76473522       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model = load_model('./model/yoon_kim_3/cnn_model_04.h5')\n",
    "\n",
    "y_predict = model.predict(test_cnn_data, batch_size=256, verbose=1)\n",
    "y_predict = np.argmax(y_predict, axis=1)\n",
    "print(classification_report(y_test, y_predict, labels = [0, 1, 2], digits=8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
