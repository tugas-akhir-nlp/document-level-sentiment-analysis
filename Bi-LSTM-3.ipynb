{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from sklearn import utils\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_train_comments = pd.read_csv(\"./corpus/prosa/data_clean_punctuation/train.csv\")\n",
    "clean_train_comments = pd.read_csv(\"./corpus/prosa/data_clean_punctuation/data_train_full.csv\")\n",
    "clean_train_comments['content'] = clean_train_comments['content'].astype('str') \n",
    "clean_train_comments['tokens'] = clean_train_comments['content'].str.split()\n",
    "clean_train_comments['sentiment'] = clean_train_comments['polarity'].astype('category').cat.codes\n",
    "   \n",
    "clean_train_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_test_comments = pd.read_csv(\"./corpus/prosa/data_clean_punctuation/test.csv\")\n",
    "clean_test_comments = pd.read_csv(\"./corpus/prosa/data_clean_punctuation/data_testing_full.csv\")\n",
    "clean_test_comments['content'] = clean_test_comments['content'].astype('str') \n",
    "clean_test_comments[\"tokens\"] = clean_test_comments[\"content\"].str.split()\n",
    "clean_test_comments['sentiment'] = clean_test_comments['polarity'].astype('category').cat.codes\n",
    "\n",
    "clean_test_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_words = [word for tokens in clean_train_comments[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in clean_train_comments[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_words = [word for tokens in clean_test_comments[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in clean_test_comments[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load('./prosa-w2v/prosa.vec')\n",
    "# word2vec = Word2Vec.load('./vectorizer/prosa/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 500 \n",
    "MAX_VOCAB_SIZE = 17872 \n",
    "MAX_SEQUENCE_LENGTH = 95\n",
    "\n",
    "#training params\n",
    "batch_size = 256\n",
    "num_epochs = 10\n",
    "hidden_size = 100\n",
    "timesteps = MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(clean_train_comments[\"content\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(clean_train_comments[\"content\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(clean_test_comments[\"content\"].tolist())\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(embeddings, max_sequence_length, num_words, embedding_dim, trainable=True):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words,\n",
    "                        embedding_dim,\n",
    "                        weights=[embeddings],\n",
    "                        input_length=max_sequence_length,\n",
    "                        trainable=trainable))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(hidden_size, input_shape=(timesteps, EMBEDDING_DIM)), merge_mode='concat'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_tr = clean_train_comments['sentiment'].values\n",
    "y_ts = clean_test_comments['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train_data\n",
    "y_train = y_tr\n",
    "\n",
    "x_test = test_data\n",
    "y_test = y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, to_categorical(y_train), epochs=num_epochs, validation_data=(x_test, to_categorical(y_test)), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save('./model/bi_lstm_3/bi_lstm_model_05.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('./model/bi_lstm_3/bi_lstm_model_05.h5')\n",
    "y_pred = model.predict(test_data)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_test, y_pred, labels = [0, 1, 2], digits=8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
