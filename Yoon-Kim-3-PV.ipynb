{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Concatenate\n",
    "import keras.layers.merge\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Doc2Vec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 700 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = 175303 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = 75 # max number of words in a comment to use\n",
    "\n",
    "#training params\n",
    "batch_size = 256  \n",
    "num_epochs = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r'\\d+','<number>',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gerindra alihkan rekomendasi ke agus an tanri ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cuci tangan pakai sunlight stelah itu pakai sa...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kasus toko obat digerebek fpi propam akan peri...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>menkeu melemah nya rupiah lebih berpengaruh pa...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>minyak jarak castor oil &lt;number&gt; ml</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content polarity\n",
       "0  gerindra alihkan rekomendasi ke agus an tanri ...  neutral\n",
       "1  cuci tangan pakai sunlight stelah itu pakai sa...  neutral\n",
       "2  kasus toko obat digerebek fpi propam akan peri...  neutral\n",
       "3  menkeu melemah nya rupiah lebih berpengaruh pa...  neutral\n",
       "4                minyak jarak castor oil <number> ml  neutral"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_comments = pd.read_csv(\"./corpus/prosa/data_clean_punctuation/train.csv\")\n",
    "# clean_train_comments = pd.read_csv(\"./corpus/prosa/data_clean_punctuation/data_train_full.csv\")\n",
    "clean_train_comments['content'] = clean_train_comments['content'].apply(preprocess)\n",
    "clean_train_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gerindra alihkan rekomendasi ke agus an tanri ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[gerindra, alihkan, rekomendasi, ke, agus, an,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cuci tangan pakai sunlight stelah itu pakai sa...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[cuci, tangan, pakai, sunlight, stelah, itu, p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kasus toko obat digerebek fpi propam akan peri...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[kasus, toko, obat, digerebek, fpi, propam, ak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>menkeu melemah nya rupiah lebih berpengaruh pa...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[menkeu, melemah, nya, rupiah, lebih, berpenga...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>minyak jarak castor oil &lt;number&gt; ml</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[minyak, jarak, castor, oil, number, ml]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content polarity  \\\n",
       "0  gerindra alihkan rekomendasi ke agus an tanri ...  neutral   \n",
       "1  cuci tangan pakai sunlight stelah itu pakai sa...  neutral   \n",
       "2  kasus toko obat digerebek fpi propam akan peri...  neutral   \n",
       "3  menkeu melemah nya rupiah lebih berpengaruh pa...  neutral   \n",
       "4                minyak jarak castor oil <number> ml  neutral   \n",
       "\n",
       "                                              tokens  sentiment  \n",
       "0  [gerindra, alihkan, rekomendasi, ke, agus, an,...          1  \n",
       "1  [cuci, tangan, pakai, sunlight, stelah, itu, p...          1  \n",
       "2  [kasus, toko, obat, digerebek, fpi, propam, ak...          1  \n",
       "3  [menkeu, melemah, nya, rupiah, lebih, berpenga...          1  \n",
       "4           [minyak, jarak, castor, oil, number, ml]          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "clean_train_comments['content'] = clean_train_comments['content'].astype('str') \n",
    "clean_train_comments.dtypes\n",
    "clean_train_comments['tokens'] = clean_train_comments['content'].apply(tokenizer.tokenize)\n",
    "clean_train_comments['sentiment'] = clean_train_comments['polarity'].astype('category').cat.codes\n",
    "   \n",
    "clean_train_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kemarin gue datang ke tempat makan baru yang a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kayak nya sih gue tidak akan mau balik lagi ke...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kalau dipikirpikir sebenarnya tidak ada yang b...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ini pertama kalinya gua ke bank buat ngurusin ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>waktu sampai dengan gue pernah disuruh ibu lat...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  polarity\n",
       "0  kemarin gue datang ke tempat makan baru yang a...  negative\n",
       "1  kayak nya sih gue tidak akan mau balik lagi ke...  negative\n",
       "2  kalau dipikirpikir sebenarnya tidak ada yang b...  negative\n",
       "3  ini pertama kalinya gua ke bank buat ngurusin ...  negative\n",
       "4  waktu sampai dengan gue pernah disuruh ibu lat...  negative"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_comments = pd.read_csv(\"./corpus/prosa/data_clean_punctuation/test.csv\")\n",
    "# clean_test_comments = pd.read_csv(\"./corpus/prosa/data_clean_punctuation/data_testing_full.csv\")\n",
    "clean_test_comments['content'] = clean_test_comments['content'].apply(preprocess)\n",
    "clean_test_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kemarin gue datang ke tempat makan baru yang a...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[kemarin, gue, datang, ke, tempat, makan, baru...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kayak nya sih gue tidak akan mau balik lagi ke...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[kayak, nya, sih, gue, tidak, akan, mau, balik...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kalau dipikirpikir sebenarnya tidak ada yang b...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[kalau, dipikirpikir, sebenarnya, tidak, ada, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ini pertama kalinya gua ke bank buat ngurusin ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[ini, pertama, kalinya, gua, ke, bank, buat, n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>waktu sampai dengan gue pernah disuruh ibu lat...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[waktu, sampai, dengan, gue, pernah, disuruh, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  polarity  \\\n",
       "0  kemarin gue datang ke tempat makan baru yang a...  negative   \n",
       "1  kayak nya sih gue tidak akan mau balik lagi ke...  negative   \n",
       "2  kalau dipikirpikir sebenarnya tidak ada yang b...  negative   \n",
       "3  ini pertama kalinya gua ke bank buat ngurusin ...  negative   \n",
       "4  waktu sampai dengan gue pernah disuruh ibu lat...  negative   \n",
       "\n",
       "                                              tokens  sentiment  \n",
       "0  [kemarin, gue, datang, ke, tempat, makan, baru...          0  \n",
       "1  [kayak, nya, sih, gue, tidak, akan, mau, balik...          0  \n",
       "2  [kalau, dipikirpikir, sebenarnya, tidak, ada, ...          0  \n",
       "3  [ini, pertama, kalinya, gua, ke, bank, buat, n...          0  \n",
       "4  [waktu, sampai, dengan, gue, pernah, disuruh, ...          0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_comments['content'] = clean_test_comments['content'].astype('str') \n",
    "clean_test_comments.dtypes\n",
    "clean_test_comments[\"tokens\"] = clean_test_comments[\"content\"].apply(tokenizer.tokenize)\n",
    "clean_test_comments['sentiment'] = clean_test_comments['polarity'].astype('category').cat.codes\n",
    "\n",
    "clean_test_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22059 words total, with a vocabulary size of 4708\n",
      "Max sentence length is 73\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in clean_train_comments[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in clean_train_comments[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1474 words total, with a vocabulary size of 607\n",
      "Max sentence length is 52\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in clean_test_comments[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in clean_test_comments[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def labelize_text(text,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(text.index, text):\n",
    "        result.append(LabeledSentence(t.split(), [prefix + '_%s' % i]))\n",
    "    return result\n",
    "  \n",
    "x_train = labelize_text(clean_train_comments[\"content\"], 'TRAIN')\n",
    "x_validation = labelize_text(clean_test_comments[\"content\"], 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load('./prosa-w2v/prosa.vec')\n",
    "# word2vec = Word2Vec.load('./vectorizer/prosa/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = pickle.load(open('./vectorizer/prosa/tfidf.pickle', 'rb'))\n",
    "model_dbow = Doc2Vec.load(\"./vectorizer/prosa/model_dbow.model\")\n",
    "model_dmc = Doc2Vec.load(\"./vectorizer/prosa/model_dmc.model\")\n",
    "model_dmm = Doc2Vec.load(\"./vectorizer/prosa/model_dmm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_doc_Vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += np.append(model_dbow[word] * tfidf[word], model_dmm[word] * tfidf[word])\n",
    "            count += 1\n",
    "        except KeyError: \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_Vector(tokens, word_size, doc_size):\n",
    "    doc_vec = build_doc_Vector(tokens, doc_size)\n",
    "    vec = np.zeros((MAX_SEQUENCE_LENGTH - len(tokens), doc_size + word_size))\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            word_vec = np.append(doc_vec, word2vec[word])\n",
    "            vec = np.append(vec, word_vec)\n",
    "        except KeyError: \n",
    "            word_vec = np.append(doc_vec, np.zeros((1, word_size)))\n",
    "            vec = np.append(vec, word_vec)\n",
    "            continue\n",
    "    vec.reshape(MAX_SEQUENCE_LENGTH, doc_size + word_size)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "935it [00:02, 334.33it/s]\n",
      "100it [00:00, 436.20it/s]\n"
     ]
    }
   ],
   "source": [
    "train_vecs = np.concatenate([[build_Vector(z, 500, 200)] for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "val_vecs = np.concatenate([[build_Vector(z, 500, 200)] for z in tqdm(map(lambda x: x.words, x_validation))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_data = len(train_vecs)\n",
    "num_data_val = len(val_vecs)\n",
    "\n",
    "train_vecs = train_vecs.reshape((num_data, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
    "val_vecs = val_vecs.reshape((num_data_val, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvNet(max_sequence_length, embedding_dim, labels_index, trainable=False, extra_conv=True):\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, embedding_dim,), dtype='float32')\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(sequence_input)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    #l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(sequence_input)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.5)(l_merge)  \n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc']) \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_tr = clean_train_comments['sentiment'].values\n",
    "y_ts = clean_test_comments['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train_vecs\n",
    "y_train = y_tr\n",
    "\n",
    "x_test = val_vecs\n",
    "y_test = y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 75, 700)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 73, 128)      268928      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 72, 128)      358528      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 71, 128)      448128      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 24, 128)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 24, 128)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 23, 128)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 71, 128)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 71, 128)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 9088)         0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          1163392     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            387         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,239,363\n",
      "Trainable params: 2,239,363\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 935 samples, validate on 100 samples\n",
      "Epoch 1/20\n",
      "935/935 [==============================] - ETA: 15s - loss: 2.5003 - acc: 0.30 - ETA: 7s - loss: 3.2057 - acc: 0.3555 - ETA: 2s - loss: 2.7511 - acc: 0.438 - 14s 15ms/step - loss: 2.6892 - acc: 0.4588 - val_loss: 2.3935 - val_acc: 0.3900\n",
      "Epoch 2/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 1.6128 - acc: 0.562 - ETA: 4s - loss: 1.4729 - acc: 0.550 - ETA: 1s - loss: 1.3341 - acc: 0.558 - 11s 12ms/step - loss: 1.2719 - acc: 0.5668 - val_loss: 1.3291 - val_acc: 0.4300\n",
      "Epoch 3/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.8500 - acc: 0.632 - ETA: 4s - loss: 0.8552 - acc: 0.627 - ETA: 1s - loss: 0.8482 - acc: 0.625 - 11s 12ms/step - loss: 0.8256 - acc: 0.6417 - val_loss: 1.2262 - val_acc: 0.3600\n",
      "Epoch 4/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.7335 - acc: 0.734 - ETA: 4s - loss: 0.7813 - acc: 0.714 - ETA: 1s - loss: 0.7381 - acc: 0.729 - 11s 12ms/step - loss: 0.7201 - acc: 0.7337 - val_loss: 1.1293 - val_acc: 0.4200\n",
      "Epoch 5/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.6266 - acc: 0.718 - ETA: 4s - loss: 0.5960 - acc: 0.752 - ETA: 1s - loss: 0.6216 - acc: 0.742 - 11s 12ms/step - loss: 0.6041 - acc: 0.7519 - val_loss: 1.2629 - val_acc: 0.4600\n",
      "Epoch 6/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.5362 - acc: 0.777 - ETA: 4s - loss: 0.5101 - acc: 0.798 - ETA: 1s - loss: 0.5019 - acc: 0.800 - 11s 12ms/step - loss: 0.5089 - acc: 0.8000 - val_loss: 1.3623 - val_acc: 0.4600\n",
      "Epoch 7/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.4586 - acc: 0.824 - ETA: 4s - loss: 0.4541 - acc: 0.826 - ETA: 1s - loss: 0.4795 - acc: 0.809 - 11s 12ms/step - loss: 0.4604 - acc: 0.8160 - val_loss: 1.3872 - val_acc: 0.4700\n",
      "Epoch 8/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.3967 - acc: 0.847 - ETA: 4s - loss: 0.4215 - acc: 0.832 - ETA: 1s - loss: 0.4146 - acc: 0.834 - 11s 12ms/step - loss: 0.4153 - acc: 0.8332 - val_loss: 1.3515 - val_acc: 0.4700\n",
      "Epoch 9/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.3701 - acc: 0.878 - ETA: 4s - loss: 0.3708 - acc: 0.867 - ETA: 1s - loss: 0.3618 - acc: 0.862 - 11s 12ms/step - loss: 0.3594 - acc: 0.8652 - val_loss: 1.1850 - val_acc: 0.4600\n",
      "Epoch 10/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.4027 - acc: 0.832 - ETA: 4s - loss: 0.3552 - acc: 0.851 - ETA: 1s - loss: 0.3267 - acc: 0.871 - 11s 12ms/step - loss: 0.3035 - acc: 0.8834 - val_loss: 1.3080 - val_acc: 0.4600\n",
      "Epoch 11/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.2461 - acc: 0.902 - ETA: 4s - loss: 0.2503 - acc: 0.912 - ETA: 1s - loss: 0.2685 - acc: 0.901 - 11s 12ms/step - loss: 0.2608 - acc: 0.9016 - val_loss: 1.4848 - val_acc: 0.4600\n",
      "Epoch 12/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.2263 - acc: 0.914 - ETA: 4s - loss: 0.2308 - acc: 0.904 - ETA: 1s - loss: 0.2263 - acc: 0.914 - 11s 12ms/step - loss: 0.2118 - acc: 0.9219 - val_loss: 1.5028 - val_acc: 0.4500\n",
      "Epoch 13/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.2637 - acc: 0.906 - ETA: 4s - loss: 0.2382 - acc: 0.910 - ETA: 1s - loss: 0.2130 - acc: 0.921 - 11s 12ms/step - loss: 0.1941 - acc: 0.9283 - val_loss: 1.7036 - val_acc: 0.4500\n",
      "Epoch 14/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.1440 - acc: 0.937 - ETA: 4s - loss: 0.1466 - acc: 0.935 - ETA: 1s - loss: 0.1536 - acc: 0.936 - 11s 12ms/step - loss: 0.1473 - acc: 0.9390 - val_loss: 1.7422 - val_acc: 0.4500\n",
      "Epoch 15/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.1355 - acc: 0.937 - ETA: 4s - loss: 0.1258 - acc: 0.947 - ETA: 1s - loss: 0.1295 - acc: 0.946 - 11s 12ms/step - loss: 0.1294 - acc: 0.9465 - val_loss: 1.6686 - val_acc: 0.4900\n",
      "Epoch 16/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.1236 - acc: 0.949 - ETA: 4s - loss: 0.1307 - acc: 0.943 - ETA: 1s - loss: 0.1217 - acc: 0.947 - 11s 12ms/step - loss: 0.1186 - acc: 0.9487 - val_loss: 1.8161 - val_acc: 0.4700\n",
      "Epoch 17/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.0977 - acc: 0.953 - ETA: 4s - loss: 0.0918 - acc: 0.959 - ETA: 1s - loss: 0.1030 - acc: 0.955 - 11s 12ms/step - loss: 0.1015 - acc: 0.9583 - val_loss: 1.9806 - val_acc: 0.4700\n",
      "Epoch 18/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.0785 - acc: 0.972 - ETA: 4s - loss: 0.0847 - acc: 0.964 - ETA: 1s - loss: 0.0888 - acc: 0.960 - 11s 12ms/step - loss: 0.0869 - acc: 0.9626 - val_loss: 1.9672 - val_acc: 0.4800\n",
      "Epoch 19/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.0846 - acc: 0.968 - ETA: 4s - loss: 0.0758 - acc: 0.972 - ETA: 1s - loss: 0.0757 - acc: 0.975 - 11s 12ms/step - loss: 0.0758 - acc: 0.9743 - val_loss: 1.9295 - val_acc: 0.5000\n",
      "Epoch 20/20\n",
      "935/935 [==============================] - ETA: 7s - loss: 0.0712 - acc: 0.976 - ETA: 4s - loss: 0.0749 - acc: 0.976 - ETA: 1s - loss: 0.0650 - acc: 0.979 - 11s 12ms/step - loss: 0.0629 - acc: 0.9797 - val_loss: 2.0446 - val_acc: 0.5100\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, to_categorical(y_train), epochs=num_epochs, validation_data=(x_test, to_categorical(y_test)), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save('./model/yoon_kim_3_pv/cnn_model_04.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 5ms/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0  0.48611111 0.87500000 0.62500000        40\n",
      "          1  0.47368421 0.45000000 0.46153846        20\n",
      "          2  0.77777778 0.17500000 0.28571429        40\n",
      "\n",
      "avg / total  0.60029240 0.51000000 0.45659341       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model = load_model('./model/yoon_kim_3_pv/cnn_model_04.h5')\n",
    "\n",
    "y_predict = model.predict(val_vecs, batch_size=256, verbose=1)\n",
    "y_predict = np.argmax(y_predict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0  0.48611111 0.87500000 0.62500000        40\n",
      "          1  0.47368421 0.45000000 0.46153846        20\n",
      "          2  0.77777778 0.17500000 0.28571429        40\n",
      "\n",
      "avg / total  0.60029240 0.51000000 0.45659341       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_predict, labels = [0, 1, 2], digits=8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
