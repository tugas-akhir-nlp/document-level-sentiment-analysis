{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Concatenate\n",
    "import keras.layers.merge\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 500 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = 175303 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = 80 # max number of words in a comment to use\n",
    "\n",
    "#training params\n",
    "batch_size = 256  \n",
    "num_epochs = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment_label(polarity):\n",
    "    if polarity=='negative':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;number&gt; ruangan itu tidak siap oleh &lt;number&gt;...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[number, ruangan, itu, tidak, siap, oleh, numb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ada tidak ada pantai dan &lt;number&gt; menit berjal...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[ada, tidak, ada, pantai, dan, number, menit, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ada tidak ada restoran karena dalam perbaikan ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[ada, tidak, ada, restoran, karena, dalam, per...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ada tidak ada restoran karena dalam perbaikan ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[ada, tidak, ada, restoran, karena, dalam, per...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ada yang bilang harga tidak boong atau ada har...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[ada, yang, bilang, harga, tidak, boong, atau,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  polarity  \\\n",
       "0   <number> ruangan itu tidak siap oleh <number>...  negative   \n",
       "1  ada tidak ada pantai dan <number> menit berjal...  negative   \n",
       "2  ada tidak ada restoran karena dalam perbaikan ...  negative   \n",
       "3  ada tidak ada restoran karena dalam perbaikan ...  negative   \n",
       "4  ada yang bilang harga tidak boong atau ada har...  negative   \n",
       "\n",
       "                                              tokens  sentiment  \n",
       "0  [number, ruangan, itu, tidak, siap, oleh, numb...          0  \n",
       "1  [ada, tidak, ada, pantai, dan, number, menit, ...          0  \n",
       "2  [ada, tidak, ada, restoran, karena, dalam, per...          0  \n",
       "3  [ada, tidak, ada, restoran, karena, dalam, per...          0  \n",
       "4  [ada, yang, bilang, harga, tidak, boong, atau,...          0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "clean_train_comments = pd.read_csv(\"./corpus/tripadvisor/train_set.csv\")\n",
    "clean_train_comments['content'] = clean_train_comments['content'].astype('str') \n",
    "clean_train_comments.dtypes\n",
    "clean_train_comments[\"tokens\"] = clean_train_comments[\"content\"].apply(tokenizer.tokenize)\n",
    "clean_train_comments['sentiment'] = clean_train_comments['polarity'].apply(sentiment_label)\n",
    "   \n",
    "clean_train_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kekecewaan untuk ritz standar menginap &lt;number...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[kekecewaan, untuk, ritz, standar, menginap, n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kekecewaan untuk ritz standar menginap &lt;number...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[kekecewaan, untuk, ritz, standar, menginap, n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kekurangan &lt;number&gt; tidak ada fasilitas apapun...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[kekurangan, number, tidak, ada, fasilitas, ap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kelebihan * lokasi strategis * breakfast stand...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[kelebihan, lokasi, strategis, breakfast, stan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kelebihan + kamar luas dan ada balkon di setia...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[kelebihan, kamar, luas, dan, ada, balkon, di,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  polarity  \\\n",
       "0  kekecewaan untuk ritz standar menginap <number...  negative   \n",
       "1  kekecewaan untuk ritz standar menginap <number...  negative   \n",
       "2  kekurangan <number> tidak ada fasilitas apapun...  negative   \n",
       "3  kelebihan * lokasi strategis * breakfast stand...  negative   \n",
       "4  kelebihan + kamar luas dan ada balkon di setia...  negative   \n",
       "\n",
       "                                              tokens  sentiment  \n",
       "0  [kekecewaan, untuk, ritz, standar, menginap, n...          0  \n",
       "1  [kekecewaan, untuk, ritz, standar, menginap, n...          0  \n",
       "2  [kekurangan, number, tidak, ada, fasilitas, ap...          0  \n",
       "3  [kelebihan, lokasi, strategis, breakfast, stan...          0  \n",
       "4  [kelebihan, kamar, luas, dan, ada, balkon, di,...          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_comments = pd.read_csv(\"./corpus/tripadvisor/test_set.csv\")\n",
    "clean_test_comments['content'] = clean_test_comments['content'].astype('str') \n",
    "clean_test_comments.dtypes\n",
    "clean_test_comments[\"tokens\"] = clean_test_comments[\"content\"].apply(tokenizer.tokenize)\n",
    "clean_test_comments['sentiment'] = clean_test_comments['polarity'].apply(sentiment_label)\n",
    "\n",
    "clean_test_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593489 words total, with a vocabulary size of 19911\n",
      "Max sentence length is 79\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in clean_train_comments[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in clean_train_comments[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113774 words total, with a vocabulary size of 7737\n",
      "Max sentence length is 71\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in clean_test_comments[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in clean_test_comments[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load('./prosa-w2v/prosa.vec')\n",
    "# word2vec = Word2Vec.load(\"./vectorizer/tripadvisor/word2vec_300.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_average_word2vec(tokens_list, vector, generate_missing=False, k=100):\n",
    "#     if len(tokens_list)<1:\n",
    "#         return np.zeros(k)\n",
    "#     if generate_missing:\n",
    "#         vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "#     else:\n",
    "#         vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "#     length = len(vectorized)\n",
    "#     summed = np.sum(vectorized, axis=0)\n",
    "#     averaged = np.divide(summed, length)\n",
    "#     return averaged\n",
    "\n",
    "# def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "#     embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "#                                                                                 generate_missing=generate_missing))\n",
    "#     return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# training_embeddings = get_word2vec_embeddings(word2vec, clean_train_comments, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19908 unique tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19909, 500)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(clean_train_comments[\"content\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(clean_train_comments[\"content\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(clean_test_comments[\"content\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False, extra_conv=True):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    #l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.5)(l_merge)  \n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc']) \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_tr = clean_train_comments['sentiment'].values\n",
    "y_ts = clean_test_comments['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_train = y_tr\n",
    "\n",
    "x_test = test_cnn_data\n",
    "y_test = y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 80, 500)      9954500     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 78, 128)      192128      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 77, 128)      256128      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 76, 128)      320128      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 26, 128)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 25, 128)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 25, 128)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 76, 128)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 76, 128)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 9728)         0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          1245312     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            129         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,968,325\n",
      "Trainable params: 2,013,825\n",
      "Non-trainable params: 9,954,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define callbacks\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=4, verbose=1)\n",
    "# callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12389 samples, validate on 2429 samples\n",
      "Epoch 1/10\n",
      "12389/12389 [==============================] - 117s 9ms/step - loss: 1.3851 - acc: 0.6063 - val_loss: 0.6710 - val_acc: 0.6196\n",
      "Epoch 2/10\n",
      "12389/12389 [==============================] - 115s 9ms/step - loss: 0.4449 - acc: 0.7993 - val_loss: 0.4517 - val_acc: 0.8028\n",
      "Epoch 3/10\n",
      "12389/12389 [==============================] - 116s 9ms/step - loss: 0.3110 - acc: 0.8776 - val_loss: 0.4958 - val_acc: 0.7872\n",
      "Epoch 4/10\n",
      "12389/12389 [==============================] - 117s 9ms/step - loss: 0.2247 - acc: 0.9131 - val_loss: 0.4555 - val_acc: 0.8168\n",
      "Epoch 5/10\n",
      "12389/12389 [==============================] - 115s 9ms/step - loss: 0.1535 - acc: 0.9422 - val_loss: 0.4728 - val_acc: 0.8110\n",
      "Epoch 6/10\n",
      "12389/12389 [==============================] - 116s 9ms/step - loss: 0.1073 - acc: 0.9626 - val_loss: 0.5325 - val_acc: 0.8267\n",
      "Epoch 7/10\n",
      "12389/12389 [==============================] - 116s 9ms/step - loss: 0.0731 - acc: 0.9741 - val_loss: 0.5851 - val_acc: 0.8143\n",
      "Epoch 8/10\n",
      "12389/12389 [==============================] - 116s 9ms/step - loss: 0.0638 - acc: 0.9784 - val_loss: 0.7264 - val_acc: 0.7970\n",
      "Epoch 9/10\n",
      "12389/12389 [==============================] - 115s 9ms/step - loss: 0.0541 - acc: 0.9823 - val_loss: 0.7195 - val_acc: 0.8082\n",
      "Epoch 10/10\n",
      "12389/12389 [==============================] - 116s 9ms/step - loss: 0.0380 - acc: 0.9870 - val_loss: 0.7351 - val_acc: 0.8131\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, epochs=num_epochs, validation_data=(x_test, y_test), batch_size=batch_size) #callbacks=callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save('./model/yoon_kim/cnn_model_09.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7350773357373297\n",
      "Test accuracy: 0.8130918072054247\n",
      "2429/2429 [==============================] - 10s 4ms/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0  0.83359498 0.81441718 0.82389449      1304\n",
      "          1  0.79047619 0.81155556 0.80087719      1125\n",
      "\n",
      "avg / total  0.81362436 0.81309181 0.81323395      2429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model = load_model('./model/yoon_kim/cnn_model_09.h5')\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "y_predict = model.predict(test_cnn_data, batch_size=256, verbose=1)\n",
    "for i in range(len(y_predict)):\n",
    "    y_predict[i][0] = round(y_predict[i][0])\n",
    "print(classification_report(y_test, y_predict, labels = [0, 1], digits=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate plots\n",
    "# plt.figure()\n",
    "# plt.plot(hist.history['loss'], lw=2.0, color='b', label='train')\n",
    "# plt.plot(hist.history['val_loss'], lw=2.0, color='r', label='val')\n",
    "# plt.title('CNN sentiment')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Cross-Entropy Loss')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(hist.history['acc'], lw=2.0, color='b', label='train')\n",
    "# plt.plot(hist.history['val_acc'], lw=2.0, color='r', label='val')\n",
    "# plt.title('CNN sentiment')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(loc='upper left')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
