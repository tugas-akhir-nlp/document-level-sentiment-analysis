{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.layers import Dense, Input, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_comments = pd.read_csv(\"./corpus/prosa/data_clean/data_train_full.csv\")\n",
    "clean_train_comments['content'] = clean_train_comments['content'].astype('str')\n",
    "clean_train_comments['sentiment'] = clean_train_comments['polarity'].astype('category').cat.codes\n",
    "   \n",
    "clean_train_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_comments = pd.read_csv(\"./corpus/prosa/data_clean/data_testing_full.csv\")\n",
    "clean_test_comments['content'] = clean_test_comments['content'].astype('str') \n",
    "clean_test_comments['sentiment'] = clean_test_comments['polarity'].astype('category').cat.codes\n",
    "\n",
    "clean_test_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sen_len = 30\n",
    "max_sents = 15\n",
    "emb_dim = 500\n",
    "\n",
    "lines_train = []\n",
    "texts_train = []\n",
    "\n",
    "lines_test = []\n",
    "texts_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data_train(text):\n",
    "    sentences = text.lower().split('.')\n",
    "    lines_train.append(sentences)  \n",
    "    text = text.lower().replace(\".\", \" \")\n",
    "    texts_train.append(text)\n",
    "    \n",
    "def prepare_data_test(text):\n",
    "    sentences = text.lower().split('.')\n",
    "    lines_test.append(sentences)  \n",
    "    text = text.lower().replace(\".\", \" \")\n",
    "    texts_test.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_train_comments['content'].apply(prepare_data_train)\n",
    "clean_test_comments['content'].apply(prepare_data_test)\n",
    "labels_train = clean_train_comments['sentiment'].tolist()\n",
    "labels_test = clean_test_comments['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = np.zeros((len(texts_train), max_sents, max_sen_len), dtype='int32')\n",
    "\n",
    "for i, sentences in enumerate(lines_train):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j< max_sents:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < max_sen_len:\n",
    "                    data_train[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "                    \n",
    "data_test = np.zeros((len(texts_test), max_sents, max_sen_len), dtype='int32')\n",
    "\n",
    "for i, sentences in enumerate(lines_test):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j< max_sents:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < max_sen_len:\n",
    "                    data_test[i, j, k] = tokenizer.word_index[word] if word in tokenizer.word_index else 0\n",
    "                    k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = data_train\n",
    "x_test = data_test\n",
    "y_train = labels_train\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load('./prosa-w2v/prosa.vec')\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, emb_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_matrix[i,:] = word2vec[word] if word in word2vec else np.random.rand(emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            emb_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_sen_len,\n",
    "                            trainable=False)\n",
    "\n",
    "sentence_input = Input(shape=(max_sen_len,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = LSTM(20)(embedded_sequences)\n",
    "sentEncoder = Model(sentence_input, l_lstm)\n",
    "\n",
    "doc_input = Input(shape=(max_sents,max_sen_len), dtype='int32')\n",
    "doc_encoder = TimeDistributed(sentEncoder)(doc_input)\n",
    "l_lstm_sent = Bidirectional(GRU(20))(doc_encoder)\n",
    "dense_1 = Dense(20, activation=\"relu\")(l_lstm_sent)\n",
    "drop_1 = Dropout(0.5)(dense_1)\n",
    "# drop_1 = Dropout(0.5)(l_lstm_sent)\n",
    "preds = Dense(3, activation='softmax')(drop_1)\n",
    "model = Model(doc_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, to_categorical(y_train), epochs=10, validation_data=(x_test, to_categorical(y_test)), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./model/lstm_bi_gru/lstm_bi_gru_model_08.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./model/lstm_bi_gru/lstm_bi_gru_model_08.h5')\n",
    "\n",
    "y_predict = model.predict(x_test, verbose=1)\n",
    "y_predict = np.argmax(y_predict, axis=1)\n",
    "print(classification_report(y_test, y_predict, labels = [0, 1, 2], digits=8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
