{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Concatenate\n",
    "import keras.layers.merge\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Doc2Vec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 700 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = 175303 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = 80 # max number of words in a comment to use\n",
    "\n",
    "#training params\n",
    "batch_size = 256  \n",
    "num_epochs = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment_label(polarity):\n",
    "    if polarity=='negative':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lokasi hotel tidak jauh dari komplek mall kali...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lokasi hotel tidak jauh dengan taksi ke area m...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lokasi hotel yang sangat strategis masih di ko...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lokasi hotel yang sangat strategis masih di ko...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lokasi hotel yang strategis di tengah kota dan...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  polarity  sentiment\n",
       "0  lokasi hotel tidak jauh dari komplek mall kali...  negative          0\n",
       "1  lokasi hotel tidak jauh dengan taksi ke area m...  negative          0\n",
       "2  lokasi hotel yang sangat strategis masih di ko...  negative          0\n",
       "3  lokasi hotel yang sangat strategis masih di ko...  negative          0\n",
       "4  lokasi hotel yang strategis di tengah kota dan...  negative          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.read_csv('./corpus/tripadvisor/test.csv')\n",
    "test_set['sentiment'] = test_set['polarity'].apply(sentiment_label)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "SEED = 2000\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(test_set['content'], test_set['sentiment'], test_size=.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# clean_train_comments = pd.read_csv(\"./corpus/tripadvisor/train_set.csv\")\n",
    "# clean_train_comments['content'] = clean_train_comments['content'].astype('str') \n",
    "# clean_train_comments.dtypes\n",
    "# clean_train_comments[\"tokens\"] = clean_train_comments[\"content\"].apply(tokenizer.tokenize)\n",
    "# clean_train_comments['sentiment'] = clean_train_comments['polarity'].apply(sentiment_label)\n",
    "   \n",
    "# clean_train_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean_test_comments = pd.read_csv(\"./corpus/tripadvisor/test_set.csv\")\n",
    "# clean_test_comments['content'] = clean_test_comments['content'].astype('str') \n",
    "# clean_test_comments.dtypes\n",
    "# clean_test_comments[\"tokens\"] = clean_test_comments[\"content\"].apply(tokenizer.tokenize)\n",
    "# clean_test_comments['sentiment'] = clean_test_comments['polarity'].apply(sentiment_label)\n",
    "\n",
    "# clean_test_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_training_words = [word for tokens in clean_train_comments[\"tokens\"] for word in tokens]\n",
    "# training_sentence_lengths = [len(tokens) for tokens in clean_train_comments[\"tokens\"]]\n",
    "# TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "# print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "# print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_test_words = [word for tokens in clean_test_comments[\"tokens\"] for word in tokens]\n",
    "# test_sentence_lengths = [len(tokens) for tokens in clean_test_comments[\"tokens\"]]\n",
    "# TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "# print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "# print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def labelize_text(text,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(text.index, text):\n",
    "        result.append(LabeledSentence(t.split(), [prefix + '_%s' % i]))\n",
    "    return result\n",
    "  \n",
    "# x_train = labelize_text(clean_train_comments[\"content\"], 'TRAIN')\n",
    "# x_validation = labelize_text(clean_test_comments[\"content\"], 'TEST')\n",
    "\n",
    "x_train = labelize_text(x_train, 'TRAIN')\n",
    "x_validation = labelize_text(x_validation, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load('./prosa-w2v/prosa.vec')\n",
    "# word2vec = Word2Vec.load('./vectorizer/prosa/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = pickle.load(open('./vectorizer/tripadvisor/tfidf.pickle', 'rb'))\n",
    "model_dbow = Doc2Vec.load(\"./vectorizer/tripadvisor/model_dbow.model\")\n",
    "model_dmc = Doc2Vec.load(\"./vectorizer/tripadvisor/model_dmc.model\")\n",
    "model_dmm = Doc2Vec.load(\"./vectorizer/tripadvisor/model_dmm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_doc_Vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += np.append(model_dbow[word] * tfidf[word], model_dmm[word] * tfidf[word])\n",
    "            count += 1\n",
    "        except KeyError: \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_Vector(tokens, word_size, doc_size):\n",
    "    doc_vec = build_doc_Vector(tokens, doc_size)\n",
    "    vec = np.zeros((MAX_SEQUENCE_LENGTH - len(tokens), doc_size + word_size))\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            word_vec = np.append(doc_vec, word2vec[word])\n",
    "            vec = np.append(vec, word_vec)\n",
    "        except KeyError: \n",
    "            word_vec = np.append(doc_vec, np.zeros((1, word_size)))\n",
    "            vec = np.append(vec, word_vec)\n",
    "            continue\n",
    "    vec.reshape(MAX_SEQUENCE_LENGTH, doc_size + word_size)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "746it [00:03, 207.13it/s]\n",
      "83it [00:00, 242.70it/s]\n"
     ]
    }
   ],
   "source": [
    "train_vecs = np.concatenate([[build_Vector(z, 500, 200)] for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "val_vecs = np.concatenate([[build_Vector(z, 500, 200)] for z in tqdm(map(lambda x: x.words, x_validation))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_data = len(train_vecs)\n",
    "num_data_val = len(val_vecs)\n",
    "\n",
    "train_vecs = train_vecs.reshape((num_data, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
    "val_vecs = val_vecs.reshape((num_data_val, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvNet(max_sequence_length, embedding_dim, labels_index, trainable=False, extra_conv=True):\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, embedding_dim,), dtype='float32')\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(sequence_input)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    #l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(sequence_input)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.5)(l_merge)  \n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc']) \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y_tr = clean_train_comments['sentiment'].values\n",
    "# y_ts = clean_test_comments['sentiment'].values\n",
    "y_tr = y_train.values\n",
    "y_ts = y_validation.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train_vecs\n",
    "y_train = y_tr\n",
    "\n",
    "x_test = val_vecs\n",
    "y_test = y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 80, 700)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 78, 128)      268928      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 77, 128)      358528      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 76, 128)      448128      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 26, 128)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 25, 128)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 25, 128)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 76, 128)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 76, 128)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 9728)         0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          1245312     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            129         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,321,025\n",
      "Trainable params: 2,321,025\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 746 samples, validate on 83 samples\n",
      "Epoch 1/20\n",
      "746/746 [==============================] - ETA: 8s - loss: 1.5880 - acc: 0.503 - ETA: 3s - loss: 4.0171 - acc: 0.546 - 11s 14ms/step - loss: 4.6708 - acc: 0.5684 - val_loss: 5.3506 - val_acc: 0.6627\n",
      "Epoch 2/20\n",
      "746/746 [==============================] - ETA: 5s - loss: 6.1626 - acc: 0.589 - ETA: 2s - loss: 5.8674 - acc: 0.599 - 10s 13ms/step - loss: 5.6746 - acc: 0.6019 - val_loss: 3.0616 - val_acc: 0.6867\n",
      "Epoch 3/20\n",
      "746/746 [==============================] - ETA: 6s - loss: 3.1399 - acc: 0.675 - ETA: 2s - loss: 2.2102 - acc: 0.664 - 10s 13ms/step - loss: 1.7767 - acc: 0.6568 - val_loss: 0.8713 - val_acc: 0.4578\n",
      "Epoch 4/20\n",
      "746/746 [==============================] - ETA: 6s - loss: 1.1551 - acc: 0.554 - ETA: 2s - loss: 1.0757 - acc: 0.576 - 10s 13ms/step - loss: 0.9890 - acc: 0.5898 - val_loss: 0.4444 - val_acc: 0.7470\n",
      "Epoch 5/20\n",
      "746/746 [==============================] - ETA: 5s - loss: 0.5203 - acc: 0.777 - ETA: 2s - loss: 0.4639 - acc: 0.798 - 10s 13ms/step - loss: 0.4323 - acc: 0.8164 - val_loss: 0.3605 - val_acc: 0.9036\n",
      "Epoch 6/20\n",
      "746/746 [==============================] - ETA: 5s - loss: 0.3723 - acc: 0.847 - ETA: 2s - loss: 0.3805 - acc: 0.843 - 10s 13ms/step - loss: 0.3567 - acc: 0.8512 - val_loss: 0.2968 - val_acc: 0.9277\n",
      "Epoch 7/20\n",
      "746/746 [==============================] - ETA: 6s - loss: 0.3229 - acc: 0.894 - ETA: 2s - loss: 0.2733 - acc: 0.900 - 10s 13ms/step - loss: 0.2682 - acc: 0.9021 - val_loss: 0.2940 - val_acc: 0.9036\n",
      "Epoch 8/20\n",
      "746/746 [==============================] - ETA: 6s - loss: 0.2198 - acc: 0.921 - ETA: 2s - loss: 0.2773 - acc: 0.904 - 10s 13ms/step - loss: 0.2553 - acc: 0.9035 - val_loss: 0.3038 - val_acc: 0.9277\n",
      "Epoch 9/20\n",
      "746/746 [==============================] - ETA: 5s - loss: 0.2792 - acc: 0.898 - ETA: 2s - loss: 0.2257 - acc: 0.914 - 10s 13ms/step - loss: 0.1965 - acc: 0.9276 - val_loss: 0.3140 - val_acc: 0.9398\n",
      "Epoch 10/20\n",
      "746/746 [==============================] - ETA: 5s - loss: 0.1710 - acc: 0.929 - ETA: 2s - loss: 0.1628 - acc: 0.927 - 10s 13ms/step - loss: 0.1682 - acc: 0.9370 - val_loss: 0.3326 - val_acc: 0.9518\n",
      "Epoch 11/20\n",
      "746/746 [==============================] - ETA: 5s - loss: 0.1128 - acc: 0.968 - ETA: 2s - loss: 0.1104 - acc: 0.959 - 10s 13ms/step - loss: 0.1272 - acc: 0.9517 - val_loss: 0.3253 - val_acc: 0.9518\n",
      "Epoch 12/20\n",
      "746/746 [==============================] - ETA: 5s - loss: 0.0944 - acc: 0.964 - ETA: 2s - loss: 0.1239 - acc: 0.951 - 10s 13ms/step - loss: 0.1282 - acc: 0.9571 - val_loss: 0.3017 - val_acc: 0.9518\n",
      "Epoch 13/20\n",
      "746/746 [==============================] - ETA: 5s - loss: 0.1015 - acc: 0.964 - ETA: 2s - loss: 0.0840 - acc: 0.972 - 10s 13ms/step - loss: 0.0859 - acc: 0.9665 - val_loss: 0.2947 - val_acc: 0.9639\n",
      "Epoch 14/20\n",
      "746/746 [==============================] - ETA: 5s - loss: 0.0931 - acc: 0.968 - ETA: 2s - loss: 0.0996 - acc: 0.962 - 10s 13ms/step - loss: 0.0901 - acc: 0.9692 - val_loss: 0.3108 - val_acc: 0.9639\n",
      "Epoch 15/20\n",
      "746/746 [==============================] - ETA: 5s - loss: 0.0659 - acc: 0.984 - ETA: 2s - loss: 0.0575 - acc: 0.984 - 10s 13ms/step - loss: 0.0707 - acc: 0.9772 - val_loss: 0.3244 - val_acc: 0.9639\n",
      "Epoch 16/20\n",
      "746/746 [==============================] - ETA: 5s - loss: 0.0510 - acc: 0.984 - ETA: 2s - loss: 0.0511 - acc: 0.984 - 10s 13ms/step - loss: 0.0531 - acc: 0.9839 - val_loss: 0.3122 - val_acc: 0.9759\n",
      "Epoch 17/20\n",
      "746/746 [==============================] - ETA: 5s - loss: 0.0539 - acc: 0.972 - ETA: 2s - loss: 0.0430 - acc: 0.982 - 10s 13ms/step - loss: 0.0426 - acc: 0.9839 - val_loss: 0.2937 - val_acc: 0.9759\n",
      "Epoch 18/20\n",
      "746/746 [==============================] - ETA: 5s - loss: 0.0473 - acc: 0.984 - ETA: 2s - loss: 0.0453 - acc: 0.984 - 10s 13ms/step - loss: 0.0391 - acc: 0.9879 - val_loss: 0.2879 - val_acc: 0.9759\n",
      "Epoch 19/20\n",
      "746/746 [==============================] - ETA: 5s - loss: 0.0200 - acc: 0.996 - ETA: 2s - loss: 0.0303 - acc: 0.994 - 10s 13ms/step - loss: 0.0314 - acc: 0.9933 - val_loss: 0.2966 - val_acc: 0.9759\n",
      "Epoch 20/20\n",
      "746/746 [==============================] - ETA: 5s - loss: 0.0163 - acc: 0.992 - ETA: 2s - loss: 0.0231 - acc: 0.990 - 10s 13ms/step - loss: 0.0229 - acc: 0.9933 - val_loss: 0.3189 - val_acc: 0.9759\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, epochs=num_epochs, validation_data=(x_test, y_test), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save('./model/yoon_kim_pv/cnn_model_04.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3189003338296729\n",
      "Test accuracy: 0.975903619484729\n",
      "83/83 [==============================] - 1s 7ms/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0  0.96491228 1.00000000 0.98214286        55\n",
      "          1  1.00000000 0.92857143 0.96296296        28\n",
      "\n",
      "avg / total  0.97674910 0.97590361 0.97567253        83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model = load_model('./model/yoon_kim_pv/cnn_model_04.h5')\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "y_predict = model.predict(x_test, batch_size=256, verbose=1)\n",
    "for i in range(len(y_predict)):\n",
    "    y_predict[i][0] = round(y_predict[i][0])\n",
    "print(classification_report(y_test, y_predict, labels = [0, 1], digits=8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
