{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Concatenate\n",
    "import keras.layers.merge\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Doc2Vec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 700 \n",
    "MAX_VOCAB_SIZE = 19911 \n",
    "MAX_SEQUENCE_LENGTH = 80 \n",
    "\n",
    "#training params\n",
    "batch_size = 256  \n",
    "num_epochs = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment_label(polarity):\n",
    "    if polarity=='negative':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('./corpus/tripadvisor/test.csv')\n",
    "test_set['sentiment'] = test_set['polarity'].apply(sentiment_label)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "SEED = 2000\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(test_set['content'], test_set['sentiment'], test_size=.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# clean_train_comments = pd.read_csv(\"./corpus/tripadvisor/train_set.csv\")\n",
    "# clean_train_comments['content'] = clean_train_comments['content'].astype('str') \n",
    "# clean_train_comments[\"tokens\"] = clean_train_comments[\"content\"].apply(tokenizer.tokenize)\n",
    "# clean_train_comments['sentiment'] = clean_train_comments['polarity'].apply(sentiment_label)\n",
    "   \n",
    "# clean_train_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean_test_comments = pd.read_csv(\"./corpus/tripadvisor/test_set.csv\")\n",
    "# clean_test_comments['content'] = clean_test_comments['content'].astype('str') \n",
    "# clean_test_comments[\"tokens\"] = clean_test_comments[\"content\"].apply(tokenizer.tokenize)\n",
    "# clean_test_comments['sentiment'] = clean_test_comments['polarity'].apply(sentiment_label)\n",
    "\n",
    "# clean_test_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_training_words = [word for tokens in clean_train_comments[\"tokens\"] for word in tokens]\n",
    "# training_sentence_lengths = [len(tokens) for tokens in clean_train_comments[\"tokens\"]]\n",
    "# TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "# print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "# print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_test_words = [word for tokens in clean_test_comments[\"tokens\"] for word in tokens]\n",
    "# test_sentence_lengths = [len(tokens) for tokens in clean_test_comments[\"tokens\"]]\n",
    "# TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "# print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "# print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize_text(text,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(text.index, text):\n",
    "        result.append(LabeledSentence(t.split(), [prefix + '_%s' % i]))\n",
    "    return result\n",
    "  \n",
    "# x_train = labelize_text(clean_train_comments[\"content\"], 'TRAIN')\n",
    "# x_validation = labelize_text(clean_test_comments[\"content\"], 'TEST')\n",
    "\n",
    "x_train = labelize_text(x_train, 'TRAIN')\n",
    "x_validation = labelize_text(x_validation, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load('./prosa-w2v/prosa.vec')\n",
    "# word2vec = Word2Vec.load('./vectorizer/prosa/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = pickle.load(open('./vectorizer/tripadvisor/tfidf.pickle', 'rb'))\n",
    "model_dbow = Doc2Vec.load(\"./vectorizer/tripadvisor/model_dbow.model\")\n",
    "model_dmc = Doc2Vec.load(\"./vectorizer/tripadvisor/model_dmc.model\")\n",
    "model_dmm = Doc2Vec.load(\"./vectorizer/tripadvisor/model_dmm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_doc_Vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += np.append(model_dbow[word] * tfidf[word], model_dmm[word] * tfidf[word])\n",
    "            count += 1\n",
    "        except KeyError: \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "def build_Vector(tokens, word_size, doc_size):\n",
    "    doc_vec = build_doc_Vector(tokens, doc_size)\n",
    "    vec = np.zeros((MAX_SEQUENCE_LENGTH - len(tokens), doc_size + word_size))\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            word_vec = np.append(doc_vec, word2vec[word])\n",
    "            vec = np.append(vec, word_vec)\n",
    "        except KeyError: \n",
    "            word_vec = np.append(doc_vec, np.zeros((1, word_size)))\n",
    "            vec = np.append(vec, word_vec)\n",
    "            continue\n",
    "    vec.reshape(MAX_SEQUENCE_LENGTH, doc_size + word_size)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = np.concatenate([[build_Vector(z, 500, 200)] for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "val_vecs = np.concatenate([[build_Vector(z, 500, 200)] for z in tqdm(map(lambda x: x.words, x_validation))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_data = len(train_vecs)\n",
    "num_data_val = len(val_vecs)\n",
    "\n",
    "train_vecs = train_vecs.reshape((num_data, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
    "val_vecs = val_vecs.reshape((num_data_val, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvNet(max_sequence_length, embedding_dim, labels_index, trainable=False, extra_conv=True):\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length, embedding_dim,), dtype='float32')\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(sequence_input)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    #l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(sequence_input)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.5)(l_merge)  \n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc']) \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y_tr = clean_train_comments['sentiment'].values\n",
    "# y_ts = clean_test_comments['sentiment'].values\n",
    "y_tr = y_train.values\n",
    "y_ts = y_validation.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train_vecs\n",
    "y_train = y_tr\n",
    "\n",
    "x_test = val_vecs\n",
    "y_test = y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x_train, y_train, epochs=num_epochs, validation_data=(x_test, y_test), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save('./model/yoon_kim_pv/cnn_model_04.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('./model/yoon_kim_pv/cnn_model_04.h5')\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "y_predict = model.predict(x_test, batch_size=256, verbose=1)\n",
    "for i in range(len(y_predict)):\n",
    "    y_predict[i][0] = round(y_predict[i][0])\n",
    "print(classification_report(y_test, y_predict, labels = [0, 1], digits=8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
